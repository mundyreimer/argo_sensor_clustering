{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T01:27:48.622835Z",
     "start_time": "2020-01-15T01:27:48.216573Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import Row, SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "from datetime import datetime\n",
    "import dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Argo data as Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argo_df = ss.read.csv(\"s3://msds-argo-clustering/argo_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Do it this way, because all nulls if define schema ahead of item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast temp as DoubleType()\n",
    "argo_df = argo_df.withColumn(\"tempTmp\", argo_df['temp'].cast(DoubleType()))\\\n",
    "                 .drop(\"temp\")\\\n",
    "                 .withColumnRenamed(\"tempTmp\", \"temp\")\\\n",
    "                 .select(\"profile_id\", \"pres\", \"temp\", \"lat\", \"lon\", \"psal\", \"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter DataFrame by conditions to keep records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argo_filterby = argo_df.groupBy(\"profile_id\") \\\n",
    "                       .agg(min(\"pres\").alias(\"min_pres\"), \n",
    "                            max(\"pres\").alias(\"max_pres\"), \n",
    "                            count(\"profile_id\").alias(\"count_profile_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, here are the profile_ids we want to keep, to be inner joined with original argo_df\n",
    "argo_keep_ids = argo_filterby.filter(\"count_profile_id >= 50 and min_pres <= 25 and max_pres >= 999\") \\\n",
    "                             .select(\"profile_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join the profile_ids to keep with original argo_df to filter and keep only desired IDs\n",
    "argo_df_keep = argo_keep_ids.join(argo_df, \"profile_id\", \"inner\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
